# -*- coding: utf-8 -*-
"""Youtube影片上架前預測頻道觀看數.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cDiRwXnUpVPopszVjcAMLPVnbEsPZURI

# Import Data and
"""

import pandas as pd
import numpy as np
import re

df = pd.read_csv('/content/df_channel_info.csv')

df.isna().sum()

def data_type_convert(df):
    df = df.fillna('None')

    # 欄位資料型態轉換
    for col in df.columns:
        if 'count' in col:
            df[col] = df[col].astype('int')

        elif 'video_length' in col:
            df['video_length'] = pd.to_timedelta(df['video_length'])

        if 'published' in col:
            df['published'] = pd.to_datetime(df['published'])

        df['description'] =  df['description'].astype('str')

    # 檢查觀看數空直(被移除的影片)
    zero_views_idx = np.where(df['view_count']==0)[0]
    df = df.drop(zero_views_idx)
    df = df.reset_index(drop=True)

    # 移除直播影片 (Outlier)
    strem_idx = np.where(df['id']=='NpdhOe67sA8')[0]
    df = df.drop(strem_idx)
    df = df.reset_index(drop=True)
    return df

def remove_non_chinese(text):

    chinese_pattern = re.compile(r'[^\u4e00-\u9fff]+')
    cleaned_text = chinese_pattern.sub('', text)
    return cleaned_text

df = data_type_convert(df)

df.info()

"""# Create New Columns"""

def df_create_col (df):

    # 哪一年
    df['year'] = df['published'].dt.year

    # 幾月
    df['month'] = df['published'].dt.month
    df['month'] = df['month'].astype('O')

    # 星期幾
    df['day_of_week'] = df['published'].dt.day_of_week
    df['day_of_week'] = df['day_of_week'].astype('O')

    # 影片上架天數
    df['on_youtybe_days'] = (pd.Timestamp.now( tz = 'UTC' ) - df['published']).dt.days

    # 資訊欄連結數量
    import re
    pattern = r'https://+'
    link_counts = []
    for i in range(len(df)):
        if df.description[i] != "None" :
            matches = re.findall(pattern, df.description[i])
        else:
            pass
        link_counts.append(len(matches))
    df['link_counts'] = link_counts

    # 有無Feat其他人
    condition = df['title'].str.contains('Ft|ft', regex=True, case=False) # the not matched case will be False
    df['feat_or_not'] = np.where(condition, 1, 0)
    df['feat_or_not'] = df['feat_or_not'].astype('O')
    # 影片總長度
    df['video_length_s'] = pd.to_timedelta(df['video_length']).dt.total_seconds()

    # 平均每天觀看數
    df['avg_day_of_views'] = df['view_count']/df['on_youtybe_days']

    # 標題字數
    df['title_text_lenght'] = df['title'].str.len()

    return df

df = df_create_col(df)

df.info()

"""# EDA"""

import seaborn as sns
import matplotlib.pyplot as plt
import ipywidgets as widgets
from IPython.display import display
sns.set(style="darkgrid")

# 檢查散佈圖的機率分布
sns.histplot(np.log(df["avg_day_of_views"]))
plt.title('The Average Views with Log_Transformation')

# 分析影片上架日期與星期
fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6),sharey = True )

df.groupby('month')['id'].count().plot(ax=axes[0],kind = 'bar',title = 'Each Month with Uploaded Videos', xlabel = 'Month')
df.groupby('day_of_week')['id'].count().plot(ax=axes[1],kind = 'bar',title = 'Each Day with Uploaded Videos', xlabel = 'Day')
df.groupby('year')['id'].count().plot(ax=axes[2],kind = 'bar',title = 'Each Year with Uploaded Videos', xlabel = 'Year')

for ax in axes:
    ax.set_ylabel('Counts')
plt.tight_layout()

# Display the subplots
plt.show()

df.columns

select_list = ['month', 'day_of_week', 'link_counts', 'feat_or_not','video_length_s','title_text_lenght']

# 用散佈圖檢視 feature and target value 之間的關係

# sns.scatterplot(x= df['title_text_lenght'], y =np.log(df['avg_day_of_views']))
# plt.xlabel('Video Length (seconds)')
# plt.ylabel('Average Views per Day')
# plt.title('Scatterplot of Video Length vs. Average Views per Day')
# plt.show()
# %matplotlib notebook

def update_plot(selected_x,Log_or_Not):
    plt.figure(figsize=(8, 6))
    if Log_or_Not == 'Log-Transform':
        sns.scatterplot(x=df[selected_x], y=np.log(df['avg_day_of_views']))
    else:
        sns.scatterplot(x=df[selected_x], y=(df['avg_day_of_views']))

    plt.xlabel(f'{selected_x}')
    plt.ylabel('Average Views per Day')
    plt.title(f'Scatterplot of {selected_x} vs. Average Views per Day ({Log_or_Not})')
    plt.show()

# Create a dropdown widget for selecting the x-axis value
x_selector = widgets.Dropdown(
    options=df[select_list],
    description='Select X-axis:',
    disabled=False
)

y_selector = widgets.Dropdown(
    options = ['Log-Transform','Original'] ,
    description='Log_or_Not:',
    disabled=False
)

# Use the interactive_output function to link the widget and update_plot function
interactive_plot = widgets.interactive_output(update_plot, {'selected_x': x_selector,
                                 'Log_or_Not': y_selector })

# Display the widget and the initial plot
display(x_selector,y_selector, interactive_plot)

df.loc[df['video_length_s'] == df['video_length_s'].max()]

"""# ML Basic Model Test
- df['feat_or_not'] contains 72 True and 357 False values, which is strongly imbalance
- Some algorithms are less sensitive to class imbalance. Decision Trees and Random Forests, for example, can handle imbalanced datasets better than others.
- One of the many qualities of Decision Trees is that they require
very little data preparation. In particular, they don’t require feature
scaling or centering at all.

## Select the Columns
"""

# Rearrange the column
train_data = df [['day_of_week','title_text_lenght', 'feat_or_not',
       'video_length_s', 'avg_day_of_views']]
# 'month', 'day_of_week', 'link_counts', 'feat_or_not','video_length_s','title_text_lenght'

train_data.info()

"""# Traing Testing split"""

X = train_data.drop(['avg_day_of_views'],axis = 1)
# y = train_data['avg_day_of_views']

y = train_data['avg_day_of_views']

X.info()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,)

X_train['feat_or_not'].value_counts()

X_test['feat_or_not'].value_counts()

"""## Split Categorical aand Numeric"""

def split_cat_numeric_columns(df):
    # Initialize empty DataFrames for categorical and numeric columns
    cat_columns = df.select_dtypes(include=['object']).copy().columns
    num_columns = df.select_dtypes(exclude=['object']).copy().columns

    return list(cat_columns), list(num_columns)

"""# Presetting"""

# Model we want to use
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor


# Column Transformer
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler

# Make pipeline
from sklearn.pipeline import Pipeline

# Evaluate
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score

"""## LinearRegression Model"""

categorical_features,numeric_features = split_cat_numeric_columns(X_train)

# Create transformers for preprocessing
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# Combine transformers using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', categorical_transformer, categorical_features),
        ('num', numeric_transformer, numeric_features)
    ])

# Create the final pipeline
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', LinearRegression())
    ]
                    )

# Evaluate the model
# Cross Validation Score
# https://medium.com/%E5%B1%95%E9%96%8B%E6%95%B8%E6%93%9A%E4%BA%BA%E7%94%9F/python%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E5%85%A5%E9%96%80-%E4%BA%8C-bbc8414d4632
neg_mses = cross_val_score(pipeline, X_train, y_train, cv=10, scoring='neg_mean_squared_error')
mses = [-i for i in neg_mses]

print('MSES:')
print(mses)
print('\nAverage RMSE:')
print(sum(np.sqrt(mses)) / len(mses))
print()
print(pipeline.fit(X_train,y_train).named_steps['preprocessor'].get_feature_names_out())

"""## Random Forest"""

from sklearn.ensemble import RandomForestRegressor
forest = RandomForestRegressor(
    # n_estimators = 200,
    # max_features = 2,
    max_depth = 3,
    # oob_score = True,
    # random_state = 42

)

# Create the final pipeline
pipeline = Pipeline(steps=[
    ('model', forest)])

# Evaluate the model
neg_mses = cross_val_score(pipeline, X_train, y_train, cv=10, scoring='neg_mean_squared_error')
mses = [-i for i in neg_mses]
print('\nAverage RMSE:')
print(sum(np.sqrt(mses)) / len(mses))

forest.fit(X_train,y_train)

y_pred = forest.predict(X_train)
np.sqrt(mean_squared_error(y_pred,y_train))

for n,i in zip(X_train.columns,forest.feature_importances_):
    print(f'{n}:{i}')

# Plot tree
from sklearn import tree
from sklearn.tree import export_graphviz
plt.figure(figsize = (15,10))
tree.plot_tree(forest.estimators_[10],filled = True,feature_names=X_train.columns)

"""# Model Selection"""

# Model we want tp use
models = []

models.append(('LR', LinearRegression()))
models.append(('SVM', SVR(kernel='linear')))
models.append(('RandomForest', RandomForestRegressor(max_depth=10)))

categorical_features,numeric_features = split_cat_numeric_columns(X_train)
# Make Column Transformer

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])


preprocessor = ColumnTransformer(
    transformers=[
        ('cat', categorical_transformer, categorical_features),
        ('num', numeric_transformer, numeric_features)
    ])

# Evaluation - baselines
num_folds = 10
seed = 7
scoring = 'neg_mean_squared_error'
results = []
names = []
for name, model in models:
    # Create the final pipeline
    if model == 'RandomForest':
        pipeline = Pipeline(steps=[
            ('model', model)])
    else:
        pipeline = Pipeline(steps=[
            ('preprocessor', preprocessor),
            ('model', model)])

    kfold = KFold(n_splits=num_folds)

    cv_results = cross_val_score(pipeline, X_train, y_train, cv=kfold, scoring=scoring)
    results.append(cv_results)
    names.append(name)
    msg = "%s RMSE %f STD %f " % (name,np.sqrt((-cv_results).mean()),(-cv_results).std())
    print(msg)

"""# We use Linear Model"""

import statsmodels.api as sm
import numpy as np

data_encoded = pd.get_dummies(X_train, columns=categorical_features, drop_first=True)
X = sm.add_constant(data_encoded)

X

feature_list =list(X.columns)
feature_list.insert(0,'const')

feature_list

model = sm.OLS(y_train,X)
results = model.fit()

print(results.summary())

"""# Test Our Model
- with log transformation on target value
- Rmoving Outlier
"""

pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', LinearRegression())])

pipeline.fit(X_train,(np.log(y_train)))

pipeline.score(X_train,(np.log(y_train)))

y_pred = pipeline.predict(X_test)

print('Final Test Error (RMSE): ',np.sqrt(mean_squared_error(np.exp(y_pred),y_test)))